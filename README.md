# ğŸ§  AI/ML Engineering with Python - Examples

This monorepo contains a collection of modular, end-to-end projects that demonstrate best practices for **AI/ML engineering** â€” from experimentation to production â€” with a focus on:

- MLOps & LLMOps patterns
- Real-world infrastructure integration
- CI/CD pipelines
- Evaluation, monitoring, and observability

These projects are designed to support a forthcoming book on **AI/ML Engineering**, and mirror the **full lifecycle of modern ML and LLM systems**.

---

## ğŸ“š What This Repo Covers

Each project explores different slices of the AI/ML engineering stack, including:

- âœ… **Training & Experimentation**: with Ray, Kedro, classic ML, notebooks
- ğŸ§  **LLM Apps & Agents**: Bedrock, CrewAI, LangGraph, RAG workflows
- ğŸš€ **Deployment & Inference**: via Ray Serve, FastAPI, Bedrock, Lambda
- ğŸ“ˆ **Monitoring & Observability**: CloudWatch, Evidently AI, MLflow
- ğŸ§ª **Evaluation**: model metrics, agent output quality, prompt tests
- ğŸ” **CI/CD Workflows**: GitHub Actions, reusable runners, CT/CI patterns
- ğŸ§° **Infrastructure**: Terraform/CDK, Docker, Python envs

---

## ğŸ—ï¸ Monorepo Structure

```text
projects/
â”œâ”€â”€ ray-ml-pipeline/           # Ray Datasets, Train, Serve
â”œâ”€â”€ kedro-ml-pipeline/         # Modular pipeline + optional MLflow
â”œâ”€â”€ bedrock-rag-agent/         # Full-stack LLM app (client + server)
â”œâ”€â”€ agent-frameworks/          # CrewAI, LangGraph, Pydantic-AI
â”œâ”€â”€ classic-ml/                # Traditional models + CI
â”œâ”€â”€ observability-pipelines/  # Lambda, CloudWatch, metrics
shared/                        # Common utilities across projects
evaluation/                    # Shared evaluation logic and runners
infra/                         # Optional shared Terraform/CDK modules
.github/                       # CI/CD workflows
