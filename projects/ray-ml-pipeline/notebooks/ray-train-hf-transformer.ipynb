{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e40b920c-8cb6-4f94-8edb-69fb17c3a213",
      "metadata": {
        "id": "e40b920c-8cb6-4f94-8edb-69fb17c3a213"
      },
      "source": [
        "# Ray Training Pipeline\n",
        "\n",
        "This notebook is based upon one of the [Ray tutorials](https://docs.ray.io/en/latest/train/examples/transformers/huggingface_text_classification.html) on the official documentation pages.\n",
        "\n",
        "We will take a dataset, train a Huggging Face transformers model on the data\n",
        "and then save the model for future inferencing downstream.\n",
        "\n",
        "You can run this notebook in Google Colab (recommended) or locally using the devcontainer setup provided in the repo folder.\n",
        "\n",
        "## Pointers\n",
        "\n",
        "\n",
        "*   If running locally and you don't have GPU, set GPU=false.\n",
        "* In general, check what memory you have available, especially if running a Docker container (OOM errors are common!).\n",
        "* If on Colab or elsewhere with free compute, have fun but remember you are still only running on a single node cluster.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -r requirements-colab.txt"
      ],
      "metadata": {
        "id": "_uc746OievL9"
      },
      "id": "_uc746OievL9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers torch mlflow"
      ],
      "metadata": {
        "id": "6hu_8V-0064f"
      },
      "id": "6hu_8V-0064f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U \"datasets==3.6.0\" \"evaluate==0.4.3\" \"transformers>=4.19.0\" \\\n",
        "# \"torch>=1.10.0\" \"mlflow==3.1.0\" \\\n",
        "# \"ray[train,tune,default]==2.47.0\" \"jupyterlab==4.4.3\" \\\n",
        "# \"jupyter-client<8\" \"notebook\" \"jupyter-kernel-gateway<2.6\" \"torchvision>=0.11.0,<0.17.0\""
      ],
      "metadata": {
        "id": "Lmf8iWf6TpU6"
      },
      "id": "Lmf8iWf6TpU6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1h8px8JrySeg"
      },
      "id": "1h8px8JrySeg"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ray"
      ],
      "metadata": {
        "id": "Cjag3VpexKFu"
      },
      "id": "Cjag3VpexKFu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d147321a-ffe0-40ef-a32e-6ba4a85cddc8",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "d147321a-ffe0-40ef-a32e-6ba4a85cddc8"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "import logging\n",
        "import ray\n",
        "\n",
        "# Defaults to machine resources\n",
        "ray.init()\n",
        "# ray.init(\n",
        "#     _memory=3 * 1024**3,              # 3 GB total usable memory\n",
        "#     object_store_memory=512 * 1024**2, # 512 MB for object store\n",
        "#     num_cpus=2,\n",
        "#     logging_level=logging.INFO\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(ray.cluster_resources())"
      ],
      "metadata": {
        "id": "8iRZDokS242I"
      },
      "id": "8iRZDokS242I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8313f44b-f0a0-4889-a269-30c5de2d6397",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "8313f44b-f0a0-4889-a269-30c5de2d6397"
      },
      "outputs": [],
      "source": [
        "use_gpu = True # set this to False to run on CPUs\n",
        "num_workers = 1  # set this to number of GPUs or CPUs you want to use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6ee2975-dcca-4102-8299-be32c6201e35",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "f6ee2975-dcca-4102-8299-be32c6201e35"
      },
      "outputs": [],
      "source": [
        "\n",
        "GLUE_TASKS = [\n",
        "    \"cola\",\n",
        "    \"mnli\",\n",
        "    \"mnli-mm\",\n",
        "    \"mrpc\",\n",
        "    \"qnli\",\n",
        "    \"qqp\",\n",
        "    \"rte\",\n",
        "    \"sst2\",\n",
        "    \"stsb\",\n",
        "    \"wnli\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a9cf7a0-daae-4855-ab76-3f86ee675dbd",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "6a9cf7a0-daae-4855-ab76-3f86ee675dbd"
      },
      "outputs": [],
      "source": [
        "task = \"cola\"\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "batch_size = 16"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03a192da-f0f7-4fca-bceb-f767cdc7d986",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "03a192da-f0f7-4fca-bceb-f767cdc7d986"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "awKDSD-KWaSM"
      },
      "id": "awKDSD-KWaSM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a480a1b7-bae8-4c0a-a806-99e8e7f39428",
      "metadata": {
        "id": "a480a1b7-bae8-4c0a-a806-99e8e7f39428"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "actual_task = \"mnli\" if task == \"mnli-mm\" else task\n",
        "datasets = load_dataset(\"glue\", actual_task, download_mode=\"force_redownload\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datasets[\"train\"][\"sentence\"][0:10]"
      ],
      "metadata": {
        "id": "Z-POFdYSrQzB"
      },
      "id": "Z-POFdYSrQzB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "122ec586-fbb0-46b6-8f9d-1a8299490e8a",
      "metadata": {
        "id": "122ec586-fbb0-46b6-8f9d-1a8299490e8a"
      },
      "source": [
        "# Process Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbfc8b19-ed9d-402e-a3d6-d38e14df8fe8",
      "metadata": {
        "id": "cbfc8b19-ed9d-402e-a3d6-d38e14df8fe8"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "372edd4c-471f-46d7-ad2d-b0b06f2482f6",
      "metadata": {
        "id": "372edd4c-471f-46d7-ad2d-b0b06f2482f6"
      },
      "outputs": [],
      "source": [
        "task_to_keys = {\n",
        "    \"cola\": (\"sentence\", None),\n",
        "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
        "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
        "    \"qnli\": (\"question\", \"sentence\"),\n",
        "    \"qqp\": (\"question1\", \"question2\"),\n",
        "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
        "    \"sst2\": (\"sentence\", None),\n",
        "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
        "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c445b07b-ced9-4e8d-9b45-5030b594882b",
      "metadata": {
        "id": "c445b07b-ced9-4e8d-9b45-5030b594882b"
      },
      "outputs": [],
      "source": [
        "import ray.data\n",
        "# Creates a ray dataset from the hugging face dataset.\n",
        "ray_datasets = {\n",
        "    \"train\": ray.data.from_huggingface(datasets[\"train\"]),\n",
        "    \"validation\": ray.data.from_huggingface(datasets[\"validation\"]),\n",
        "    \"test\": ray.data.from_huggingface(datasets[\"test\"]),\n",
        "}\n",
        "ray_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02ceef25-2c68-4f60-bee8-04ebe3106d81",
      "metadata": {
        "id": "02ceef25-2c68-4f60-bee8-04ebe3106d81"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# Tokenize input sentences\n",
        "def collate_fn(examples: dict[str, np.array]):\n",
        "    sentence1_key, sentence2_key = task_to_keys[task]\n",
        "    if sentence2_key is None:\n",
        "        outputs = tokenizer(\n",
        "            list(examples[sentence1_key]),\n",
        "            truncation=True,\n",
        "            padding=\"longest\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "    else:\n",
        "        outputs = tokenizer(\n",
        "            list(examples[sentence1_key]),\n",
        "            list(examples[sentence2_key]),\n",
        "            truncation=True,\n",
        "            padding=\"longest\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "    outputs[\"labels\"] = torch.LongTensor(examples[\"label\"])\n",
        "\n",
        "    # Move all tensors to CPU (or GPU if available)\n",
        "    for key, value in outputs.items():\n",
        "        outputs[key] = value.to(device)\n",
        "\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# False because I specified the Ray cluster as CPU above?\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "7WvHKJjmsrE1"
      },
      "id": "7WvHKJjmsrE1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "74a1c6f9-8267-4033-aa22-2e1e206b7604",
      "metadata": {
        "id": "74a1c6f9-8267-4033-aa22-2e1e206b7604"
      },
      "source": [
        "## Fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer"
      ],
      "metadata": {
        "id": "GykgBpK7caSL"
      },
      "id": "GykgBpK7caSL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers"
      ],
      "metadata": {
        "id": "ApevHpyJs0VS"
      },
      "id": "ApevHpyJs0VS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformers.__version__"
      ],
      "metadata": {
        "id": "M_rEOYO3s_Zv"
      },
      "id": "M_rEOYO3s_Zv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "TpFkEnjv7nsK"
      },
      "id": "TpFkEnjv7nsK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24edcf06-cc3a-497c-8e99-6a543a7dffed",
      "metadata": {
        "id": "24edcf06-cc3a-497c-8e99-6a543a7dffed"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import evaluate\n",
        "\n",
        "\n",
        "import ray.train\n",
        "from ray.train.huggingface.transformers import prepare_trainer, RayTrainReportCallback\n",
        "\n",
        "num_labels = 3 if task.startswith(\"mnli\") else 1 if task == \"stsb\" else 2\n",
        "metric_name = (\n",
        "    \"pearson\"\n",
        "    if task == \"stsb\"\n",
        "    else \"matthews_correlation\"\n",
        "    if task == \"cola\"\n",
        "    else \"accuracy\"\n",
        ")\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "validation_key = (\n",
        "    \"validation_mismatched\"\n",
        "    if task == \"mnli-mm\"\n",
        "    else \"validation_matched\"\n",
        "    if task == \"mnli\"\n",
        "    else \"validation\"\n",
        ")\n",
        "name = f\"{model_name}-finetuned-{task}\"\n",
        "\n",
        "# Calculate the maximum steps per epoch based on the number of rows in the training dataset.\n",
        "# Make sure to scale by the total number of training workers and the per device batch size.\n",
        "max_steps_per_epoch = ray_datasets[\"train\"].count() // (batch_size * num_workers)\n",
        "\n",
        "\n",
        "def train_func(config):\n",
        "    print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "    metric = evaluate.load(\"glue\", actual_task)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_checkpoint, num_labels=num_labels\n",
        "    )\n",
        "\n",
        "    train_ds = ray.train.get_dataset_shard(\"train\")\n",
        "    eval_ds = ray.train.get_dataset_shard(\"eval\")\n",
        "\n",
        "    train_ds_iterable = train_ds.iter_torch_batches(\n",
        "        batch_size=batch_size, collate_fn=collate_fn\n",
        "    )\n",
        "    eval_ds_iterable = eval_ds.iter_torch_batches(\n",
        "        batch_size=batch_size, collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    print(\"max_steps_per_epoch: \", max_steps_per_epoch)\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        name,\n",
        "        # evaluation_strategy=\"epoch\", # Removed as per error\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_strategy=\"epoch\",\n",
        "        per_device_train_batch_size=config.get(\"batch_size\", batch_size),\n",
        "        per_device_eval_batch_size=config.get(\"batch_size\", batch_size),\n",
        "        learning_rate=config.get(\"learning_rate\", 2e-5),\n",
        "        num_train_epochs=config.get(\"epochs\", 2),\n",
        "        weight_decay=config.get(\"weight_decay\", 0.01),\n",
        "        push_to_hub=False,\n",
        "        max_steps=max_steps_per_epoch * config.get(\"epochs\", 2),\n",
        "        disable_tqdm=True,  # declutter the output a little\n",
        "        no_cuda=not use_gpu,  # you need to explicitly set no_cuda if you want CPUs\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "        predictions, labels = eval_pred\n",
        "        if task != \"stsb\":\n",
        "            predictions = np.argmax(predictions, axis=1)\n",
        "        else:\n",
        "            predictions = predictions[:, 0]\n",
        "        return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model,\n",
        "        args,\n",
        "        train_dataset=train_ds_iterable,\n",
        "        eval_dataset=eval_ds_iterable,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    trainer.add_callback(RayTrainReportCallback())\n",
        "\n",
        "    trainer = prepare_trainer(trainer)\n",
        "\n",
        "    print(\"Starting training\")\n",
        "    trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f95a41d3-f523-4ee0-a0de-874db45af411",
      "metadata": {
        "id": "f95a41d3-f523-4ee0-a0de-874db45af411"
      },
      "outputs": [],
      "source": [
        "from ray.train.torch import TorchTrainer\n",
        "from ray.train import RunConfig, ScalingConfig, CheckpointConfig\n",
        "\n",
        "trainer = TorchTrainer(\n",
        "    train_func,\n",
        "    scaling_config=ScalingConfig(num_workers=num_workers, use_gpu=use_gpu),\n",
        "    datasets={\n",
        "        \"train\": ray_datasets[\"train\"],\n",
        "        \"eval\": ray_datasets[\"validation\"],\n",
        "    },\n",
        "    run_config=RunConfig(\n",
        "        checkpoint_config=CheckpointConfig(\n",
        "            num_to_keep=1,\n",
        "            checkpoint_score_attribute=\"eval_loss\",\n",
        "            checkpoint_score_order=\"min\",\n",
        "        ),\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b1dc6e4-48c7-4a2b-8ea4-e7ea34aa8800",
      "metadata": {
        "id": "4b1dc6e4-48c7-4a2b-8ea4-e7ea34aa8800",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "result = trainer.fit()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "NRPqp7xq9bue"
      },
      "id": "NRPqp7xq9bue",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Inference"
      ],
      "metadata": {
        "id": "AeLv6ABI9sX3"
      },
      "id": "AeLv6ABI9sX3"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Path to your Ray checkpoint\n",
        "checkpoint_path = result.checkpoint.path\n",
        "\n",
        "# Hugging Face saves model files into 'model' subdir by default\n",
        "model_path = os.path.join(checkpoint_path, \"checkpoint\")\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "model.eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "QxvYv67R9cbc"
      },
      "id": "QxvYv67R9cbc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example sentences (adjust to your task: sentiment, NLI, etc.)\n",
        "sentences = [\n",
        "    \"This movie was absolutely fantastic!\",\n",
        "    \"I wouldn't recommend this book to anyone.\",\n",
        "    \"the book is the table on\",\n",
        "    \"the whole hands are in his world\",\n",
        "    \"door close the please\",\n",
        "    \"gr fasdjfkn jadskfnaf\",\n",
        "    \"alskdjflksadjflkjasdf\",\n",
        "    \"The banana is sleeping.\",\n",
        "    \"This are book.\"\n",
        "]\n",
        "\n",
        "# Tokenize\n",
        "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Inference\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "print(\"Predicted labels:\", predictions.cpu().numpy())"
      ],
      "metadata": {
        "id": "dH9_D74h-Ess"
      },
      "id": "dH9_D74h-Ess",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}