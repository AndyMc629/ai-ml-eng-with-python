{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e40b920c-8cb6-4f94-8edb-69fb17c3a213",
      "metadata": {
        "id": "e40b920c-8cb6-4f94-8edb-69fb17c3a213"
      },
      "source": [
        "# Ray Training Pipeline\n",
        "\n",
        "This notebook is based upon one of the [Ray tutorials](https://docs.ray.io/en/latest/train/examples/transformers/huggingface_text_classification.html) on the official documentation pages.\n",
        "\n",
        "We will take a dataset, train a Huggging Face transformers model on the data\n",
        "and then save the model for future inferencing downstream.\n",
        "\n",
        "You can run this notebook in Google Colab (recommended) or locally using the devcontainer setup provided in the repo folder.\n",
        "\n",
        "## Pointers\n",
        "\n",
        "\n",
        "*   If running locally and you don't have GPU, set GPU=false.\n",
        "* In general, check what memory you have available, especially if running a Docker container (OOM errors are common!).\n",
        "* If on Colab or elsewhere with free compute, have fun but remember you are still only running on a single node cluster.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "metadata": {
        "id": "02ox-E4tdp5l",
        "outputId": "cea80218-b2c0-4d98-fe15-ad0c9b788e5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "id": "02ox-E4tdp5l",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'NoneType' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-2535520808>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mwhile\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'complete'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     result = _output.eval_js(\n\u001b[1;32m    173\u001b[0m         'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -r requirements-colab.txt"
      ],
      "metadata": {
        "id": "_uc746OievL9"
      },
      "id": "_uc746OievL9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \"datasets==3.6.0\" \"evaluate==0.4.3\" \"transformers>=4.19.0\" \\\n",
        "\"torch>=1.10.0\" \"mlflow==3.1.0\" \\\n",
        "\"ray[train,tune,default]==2.47.0\" \"jupyterlab==4.4.3\" \\\n",
        "\"jupyter-client<8\" \"notebook\" \"jupyter-kernel-gateway<2.6\" \"torchvision>=0.11.0,<0.17.0\""
      ],
      "metadata": {
        "id": "Lmf8iWf6TpU6"
      },
      "id": "Lmf8iWf6TpU6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d147321a-ffe0-40ef-a32e-6ba4a85cddc8",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "d147321a-ffe0-40ef-a32e-6ba4a85cddc8"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "import logging\n",
        "import ray\n",
        "\n",
        "ray.init(\n",
        "    _memory=3 * 1024**3,              # 3 GB total usable memory\n",
        "    object_store_memory=512 * 1024**2, # 512 MB for object store\n",
        "    num_cpus=2,\n",
        "    logging_level=logging.INFO\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d07e068-9be1-40a8-9126-3b388416b055",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "7d07e068-9be1-40a8-9126-3b388416b055"
      },
      "outputs": [],
      "source": [
        "pprint(ray.cluster_resources())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8313f44b-f0a0-4889-a269-30c5de2d6397",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "8313f44b-f0a0-4889-a269-30c5de2d6397"
      },
      "outputs": [],
      "source": [
        "use_gpu = False  # set this to False to run on CPUs\n",
        "num_workers = 1  # set this to number of GPUs or CPUs you want to use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6ee2975-dcca-4102-8299-be32c6201e35",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "f6ee2975-dcca-4102-8299-be32c6201e35"
      },
      "outputs": [],
      "source": [
        "\n",
        "GLUE_TASKS = [\n",
        "    \"cola\",\n",
        "    \"mnli\",\n",
        "    \"mnli-mm\",\n",
        "    \"mrpc\",\n",
        "    \"qnli\",\n",
        "    \"qqp\",\n",
        "    \"rte\",\n",
        "    \"sst2\",\n",
        "    \"stsb\",\n",
        "    \"wnli\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a9cf7a0-daae-4855-ab76-3f86ee675dbd",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "6a9cf7a0-daae-4855-ab76-3f86ee675dbd"
      },
      "outputs": [],
      "source": [
        "task = \"cola\"\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "batch_size = 16"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03a192da-f0f7-4fca-bceb-f767cdc7d986",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "03a192da-f0f7-4fca-bceb-f767cdc7d986"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "awKDSD-KWaSM"
      },
      "id": "awKDSD-KWaSM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a480a1b7-bae8-4c0a-a806-99e8e7f39428",
      "metadata": {
        "id": "a480a1b7-bae8-4c0a-a806-99e8e7f39428"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "actual_task = \"mnli\" if task == \"mnli-mm\" else task\n",
        "datasets = load_dataset(\"glue\", actual_task)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "122ec586-fbb0-46b6-8f9d-1a8299490e8a",
      "metadata": {
        "id": "122ec586-fbb0-46b6-8f9d-1a8299490e8a"
      },
      "source": [
        "# Process Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbfc8b19-ed9d-402e-a3d6-d38e14df8fe8",
      "metadata": {
        "id": "cbfc8b19-ed9d-402e-a3d6-d38e14df8fe8"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "372edd4c-471f-46d7-ad2d-b0b06f2482f6",
      "metadata": {
        "id": "372edd4c-471f-46d7-ad2d-b0b06f2482f6"
      },
      "outputs": [],
      "source": [
        "task_to_keys = {\n",
        "    \"cola\": (\"sentence\", None),\n",
        "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
        "    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
        "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
        "    \"qnli\": (\"question\", \"sentence\"),\n",
        "    \"qqp\": (\"question1\", \"question2\"),\n",
        "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
        "    \"sst2\": (\"sentence\", None),\n",
        "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
        "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c445b07b-ced9-4e8d-9b45-5030b594882b",
      "metadata": {
        "id": "c445b07b-ced9-4e8d-9b45-5030b594882b"
      },
      "outputs": [],
      "source": [
        "import ray.data\n",
        "\n",
        "ray_datasets = {\n",
        "    \"train\": ray.data.from_huggingface(datasets[\"train\"]),\n",
        "    \"validation\": ray.data.from_huggingface(datasets[\"validation\"]),\n",
        "    \"test\": ray.data.from_huggingface(datasets[\"test\"]),\n",
        "}\n",
        "ray_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02ceef25-2c68-4f60-bee8-04ebe3106d81",
      "metadata": {
        "id": "02ceef25-2c68-4f60-bee8-04ebe3106d81"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# Tokenize input sentences\n",
        "def collate_fn(examples: dict[str, np.array]):\n",
        "    sentence1_key, sentence2_key = task_to_keys[task]\n",
        "    if sentence2_key is None:\n",
        "        outputs = tokenizer(\n",
        "            list(examples[sentence1_key]),\n",
        "            truncation=True,\n",
        "            padding=\"longest\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "    else:\n",
        "        outputs = tokenizer(\n",
        "            list(examples[sentence1_key]),\n",
        "            list(examples[sentence2_key]),\n",
        "            truncation=True,\n",
        "            padding=\"longest\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "    outputs[\"labels\"] = torch.LongTensor(examples[\"label\"])\n",
        "\n",
        "    # Move all tensors to CPU (or GPU if available)\n",
        "    for key, value in outputs.items():\n",
        "        outputs[key] = value.to(device)\n",
        "\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74a1c6f9-8267-4033-aa22-2e1e206b7604",
      "metadata": {
        "id": "74a1c6f9-8267-4033-aa22-2e1e206b7604"
      },
      "source": [
        "## Fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer"
      ],
      "metadata": {
        "id": "GykgBpK7caSL"
      },
      "id": "GykgBpK7caSL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24edcf06-cc3a-497c-8e99-6a543a7dffed",
      "metadata": {
        "id": "24edcf06-cc3a-497c-8e99-6a543a7dffed"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import evaluate\n",
        "\n",
        "\n",
        "import ray.train\n",
        "from ray.train.huggingface.transformers import prepare_trainer, RayTrainReportCallback\n",
        "\n",
        "num_labels = 3 if task.startswith(\"mnli\") else 1 if task == \"stsb\" else 2\n",
        "metric_name = (\n",
        "    \"pearson\"\n",
        "    if task == \"stsb\"\n",
        "    else \"matthews_correlation\"\n",
        "    if task == \"cola\"\n",
        "    else \"accuracy\"\n",
        ")\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "validation_key = (\n",
        "    \"validation_mismatched\"\n",
        "    if task == \"mnli-mm\"\n",
        "    else \"validation_matched\"\n",
        "    if task == \"mnli\"\n",
        "    else \"validation\"\n",
        ")\n",
        "name = f\"{model_name}-finetuned-{task}\"\n",
        "\n",
        "# Calculate the maximum steps per epoch based on the number of rows in the training dataset.\n",
        "# Make sure to scale by the total number of training workers and the per device batch size.\n",
        "max_steps_per_epoch = ray_datasets[\"train\"].count() // (batch_size * num_workers)\n",
        "\n",
        "\n",
        "def train_func(config):\n",
        "    print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "    metric = evaluate.load(\"glue\", actual_task)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_checkpoint, num_labels=num_labels\n",
        "    )\n",
        "\n",
        "    train_ds = ray.train.get_dataset_shard(\"train\")\n",
        "    eval_ds = ray.train.get_dataset_shard(\"eval\")\n",
        "\n",
        "    train_ds_iterable = train_ds.iter_torch_batches(\n",
        "        batch_size=batch_size, collate_fn=collate_fn\n",
        "    )\n",
        "    eval_ds_iterable = eval_ds.iter_torch_batches(\n",
        "        batch_size=batch_size, collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    print(\"max_steps_per_epoch: \", max_steps_per_epoch)\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        name,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_strategy=\"epoch\",\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        learning_rate=config.get(\"learning_rate\", 2e-5),\n",
        "        num_train_epochs=config.get(\"epochs\", 2),\n",
        "        weight_decay=config.get(\"weight_decay\", 0.01),\n",
        "        push_to_hub=False,\n",
        "        max_steps=max_steps_per_epoch * config.get(\"epochs\", 2),\n",
        "        disable_tqdm=True,  # declutter the output a little\n",
        "        no_cuda=not use_gpu,  # you need to explicitly set no_cuda if you want CPUs\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "        predictions, labels = eval_pred\n",
        "        if task != \"stsb\":\n",
        "            predictions = np.argmax(predictions, axis=1)\n",
        "        else:\n",
        "            predictions = predictions[:, 0]\n",
        "        return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model,\n",
        "        args,\n",
        "        train_dataset=train_ds_iterable,\n",
        "        eval_dataset=eval_ds_iterable,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    trainer.add_callback(RayTrainReportCallback())\n",
        "\n",
        "    trainer = prepare_trainer(trainer)\n",
        "\n",
        "    print(\"Starting training\")\n",
        "    trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f95a41d3-f523-4ee0-a0de-874db45af411",
      "metadata": {
        "id": "f95a41d3-f523-4ee0-a0de-874db45af411"
      },
      "outputs": [],
      "source": [
        "from ray.train.torch import TorchTrainer\n",
        "from ray.train import RunConfig, ScalingConfig, CheckpointConfig\n",
        "\n",
        "trainer = TorchTrainer(\n",
        "    train_func,\n",
        "    scaling_config=ScalingConfig(num_workers=num_workers, use_gpu=use_gpu),\n",
        "    datasets={\n",
        "        \"train\": ray_datasets[\"train\"],\n",
        "        \"eval\": ray_datasets[\"validation\"],\n",
        "    },\n",
        "    run_config=RunConfig(\n",
        "        checkpoint_config=CheckpointConfig(\n",
        "            num_to_keep=1,\n",
        "            checkpoint_score_attribute=\"eval_loss\",\n",
        "            checkpoint_score_order=\"min\",\n",
        "        ),\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b1dc6e4-48c7-4a2b-8ea4-e7ea34aa8800",
      "metadata": {
        "id": "4b1dc6e4-48c7-4a2b-8ea4-e7ea34aa8800"
      },
      "outputs": [],
      "source": [
        "result = trainer.fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a25340b-18f7-4c16-9110-22b3aecd5cc3",
      "metadata": {
        "id": "4a25340b-18f7-4c16-9110-22b3aecd5cc3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}